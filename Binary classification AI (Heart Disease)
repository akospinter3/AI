{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.12.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":125192,"databundleVersionId":15408205,"sourceType":"competition"},{"sourceId":14111592,"sourceType":"datasetVersion","datasetId":8988973}],"dockerImageVersionId":31259,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"source":"<a href=\"https://www.kaggle.com/code/kospintr/heart-xgb-lightgbm-catb-baseline-k-fold?scriptVersionId=295804412\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown"},{"cell_type":"markdown","source":"# Overview and main features","metadata":{}},{"cell_type":"markdown","source":"**1. Import packages**\n\n**2. Load and explore data:**\n* Load and split data into train and validation sets with a stratified (*STRAT == True*) or simple K-Fold splitter. As reference for the optional stratification either a calculated multilabel (based on selected features and/or labels) or simply the label can be used (*EXTENDED_STRAT == False*).\n* External dataset can be merged with the train dataset (*ADD_EXTERN_DATA == True*).\n* Calculate statistical features (mean and count).\n* List of dataset columns including data types and number of non-zero elements.\n* Show number of unique elements of categorical features.\n* Show probability distribution of numerical features.\n* Compare probabilty distribution of numerical features between train, validation and test sets.\n\n**3. Preprocess data:**\n* The dataset includes 2 main types of data: numerical features and categorical features.\n* Preprocessing pipelines include an imputing step (for robustness of pipeline, currently there are no missing values in raw data), a transforming step and a scaling step (if necessary).\n* Selected preprocessing pipelines will be merged with a ColumnTransformer, possible pipelines are:\n    * MinMax/standard/robust scaling of numerical features\n    * Principal component analysis (PCA) of numerical features\n    * Logaritmic/square/cube/square root/cube root transformation of numerical features\n    * KBins discretization of numerical features\n    * Ordinal/onehot encoding of categorical features\n    * Statistical features (frequency of feature values / mean and count of target values for feature values) for both numerical and categorical features.\n* Optionally, preprocessed features will be passed into a feature selection model to extract most relevant features. The maximum number of features can be configured (*MAX_FEAT*).\n\n**4. Define model space for ML methods:**\n* Definition of model space with a large number of possible ML methods:\n    * Linear models: SGD\n    * Support vector machine model: SVC\n    * Ensemble models: RandomForest / GradientBoosting / AdaBoost / HistGradientBoosting / XGB / LGBM / CatBoost\n    * Other models: KNeighbors\n* Optionally, GPU acceleration can be activated (if supported by choosen ML method(s)) by *GPU_ACC*.\n\n**5. Training and evaluation:**\n* Definition of model parameter sets and tuner interfaces.\n* Simple fitting or tuning (*TUNING == True*) of choosen models (*EST_IDS*) on train dataset with K-Fold method (*FOLD == k*).\n* Early stopping and the usage of category data type can be configured (*EST_IDS_W_EARLYSTOPPING/EST_IDS_W_CAT_FEAT*).\n\n**6. Evaluation:**\n* Calculate ROC_AUC scores for all estimators on both train and validation dataset.\n* ROC_AUC score on mean value of ensemble predictions for each fold on validation dataset.\n* Show feature importances.\n* Show ROC_AUC scores of subcategory subsets.\n* Show worse ROC_AUC scores of multicategory subsets\n\n**7. Submission:**\n* Predict labels and create submission.csv file.\n* If more than 1 model have been choosen in EST_IDS (Ensemble solution) then the prediction will be an average of the predictions of the single models.","metadata":{}},{"cell_type":"markdown","source":"# 1. Import packages","metadata":{}},{"cell_type":"code","source":"## Import packages\n\n# General purpose modules\nimport time\nfrom copy import deepcopy\nimport warnings\n\n# Data handling and visualization modules\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport cupy as cp\n\n# Skikit-learn preprocessing and evaluation modules\nfrom sklearn.feature_selection import SelectFromModel\nfrom sklearn.model_selection import ShuffleSplit\nfrom sklearn.model_selection import StratifiedShuffleSplit\nfrom sklearn.model_selection import KFold\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.compose import make_column_selector\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.preprocessing import FunctionTransformer\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.preprocessing import RobustScaler\nfrom sklearn.preprocessing import OrdinalEncoder\nfrom sklearn.preprocessing import OneHotEncoder\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.preprocessing import KBinsDiscretizer\nfrom sklearn.metrics import roc_auc_score\n\n# Skikit-learn ML modules\nfrom sklearn.decomposition import PCA\nfrom sklearn.svm import SVC\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\nfrom sklearn.ensemble import HistGradientBoostingClassifier, AdaBoostClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.linear_model import SGDClassifier\n\n# Further ML modules\nimport xgboost as xgboost\nimport lightgbm as lightgbm\nfrom catboost import CatBoostClassifier","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-04T08:08:15.449335Z","iopub.execute_input":"2026-02-04T08:08:15.449622Z","iopub.status.idle":"2026-02-04T08:08:26.520801Z","shell.execute_reply.started":"2026-02-04T08:08:15.449595Z","shell.execute_reply":"2026-02-04T08:08:26.519753Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# 2. Load and explore data","metadata":{}},{"cell_type":"code","source":"## Read csv files and spliting the data into training and validation set\n\nADD_EXTERN_DATA = False\nSTRAT = False # Use stratification for data spliting\nEXTENDED_STRAT = False # Stratification is based on multiple features\n\n# Read csv files\ntrainval = pd.read_csv('/kaggle/input/playground-series-s6e2/train.csv')\nextern_data = pd.read_csv('/kaggle/input/heartdisease/Heart_Disease_Prediction.csv')\nif ADD_EXTERN_DATA:\n    trainval = pd.concat([trainval[trainval.columns[1:]], extern_data[trainval.columns[1:]]]\n                         ).reset_index(drop=True).reset_index().rename(columns={'index':'id'})\ntest = pd.read_csv('/kaggle/input/playground-series-s6e2/test.csv')\n\n# Discretization of labels and numeric features\ntarget = 'Heart Disease'\ntrainval[target] = LabelEncoder().fit_transform(trainval[target]).astype(np.uint8)\n\n# Calculate statistical features for all columns\nglobal_stats = {'mean': trainval[target].mean(), 'count': 0}\nfreq_encodings = {}\nstats_mean = {}\nstats_count = {}\nfor c in trainval.columns[1:-1]:\n    freq_encodings[c] = trainval[c].value_counts(normalize=True).to_dict()\n    stats_mean[c] = trainval.groupby(c)[target].agg(['mean']).to_dict()['mean']\n    stats_count[c] = trainval.groupby(c)[target].agg(['count']).to_dict()['count']\n\n# Determine stratification bins\nstrat_encoder = LabelEncoder()\nstrat_encoder_eval = LabelEncoder()\nstrat_cols = ['Thallium', 'Chest pain type', target]\nstrat_cols_eval = ['Thallium', 'Chest pain type', 'Number of vessels fluro']\ntrainval['multicat'] = strat_encoder.fit_transform(trainval[strat_cols].astype(str).agg('_'.join, axis=1))\ntrainval['multicat_eval'] = strat_encoder_eval.fit_transform(trainval[strat_cols_eval].astype(str).agg('_'.join, axis=1))\nsss = (StratifiedShuffleSplit if STRAT else ShuffleSplit)(n_splits=1, test_size=0.01, random_state=42)\n\n# Spliting data\ntrain_idx, val_idx = next(sss.split(trainval, trainval['multicat'] if EXTENDED_STRAT else trainval[target]))\ntrain = trainval.iloc[train_idx].reset_index()\nval = trainval.iloc[val_idx].reset_index()\ntrainval_labels = trainval.pop(target)\ntrain_labels = train.pop(target)\nval_labels = val.pop(target)\n\n# Verify sizes\nprint(f\"Total rows:   {len(trainval)}\")\nprint(f\"Dev train:    {len(train)} ({len(train)/len(trainval):.2%})\")\nprint(f\"Dev valid:    {len(val)} ({len(val)/len(trainval):.2%})\")\nprint(f\"Number of unique elements in multicat column: {len(trainval['multicat'].unique())}\")\nprint('-'*80, end='\\n\\n')\n\n# Size of stratification bins\nprint(trainval['multicat'].value_counts().tail())\nprint('-'*80, end='\\n\\n')\nprint(trainval['multicat_eval'].value_counts().tail())","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-04T08:08:26.523336Z","iopub.execute_input":"2026-02-04T08:08:26.523977Z","iopub.status.idle":"2026-02-04T08:08:35.771695Z","shell.execute_reply.started":"2026-02-04T08:08:26.523941Z","shell.execute_reply":"2026-02-04T08:08:35.77062Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"## Explore train dataset\n\nprint('List of dataset columns including data types and number of non-zero elements: ', end='\\n\\n')\ntrain.info()\nprint('-'*80, end='\\n\\n')\n\n# Explore categorical features\ncat_columns = ['Sex', 'Chest pain type', 'FBS over 120', 'EKG results', 'Exercise angina', 'Slope of ST',\n              'Number of vessels fluro', 'Thallium']\nprint('Number of unique elements of categorical features: ', end='\\n\\n')\nfor cat in cat_columns:\n    print(train[cat].value_counts(), end='\\n\\n')\n\n# Explore numerical features\nnum_columns = ['Age', 'BP', 'Cholesterol', 'Max HR', 'ST depression']\ntrain[num_columns].hist(bins=100, figsize=(16,10))\nplt.suptitle('Probability distribution of numerical features: ')\nprint('-'*80, end='\\n\\n')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-04T08:08:35.772767Z","iopub.execute_input":"2026-02-04T08:08:35.773036Z","iopub.status.idle":"2026-02-04T08:08:37.529996Z","shell.execute_reply.started":"2026-02-04T08:08:35.773011Z","shell.execute_reply":"2026-02-04T08:08:37.528974Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"## Compare probabilty distribution of numerical features between train, validation and test sets\n\ndf_plot = pd.concat([train[num_columns].assign(Set='Train'), val[num_columns].assign(Set='Validation'),\n                     test[num_columns].assign(Set='Test')])\ndf_plot.insert(5, value=pd.concat([train_labels, val_labels, pd.Series([None] * len(test), name=target)]), column=target)\ndf_plot.reset_index(drop=True, inplace=True)\nwarnings.filterwarnings(\"ignore\", category=FutureWarning, module=\"seaborn\") # Suppress the specific FutureWarning\n\nn_cols = 3\nn_rows = (len(num_columns) + n_cols - 1) // n_cols\nfig, axes = plt.subplots(n_rows, n_cols, figsize=(15, 4 * n_rows))\nfig.suptitle('Feature distributions in train and validation sets')\naxes = axes.flatten()\nfor i in range(len(num_columns), len(axes)):\n    axes[i].axis('off')\nfor i, col in enumerate(df_plot.columns[:-1]):\n    sns.kdeplot(data=df_plot, x=col, ax=axes[i], hue='Set', common_norm=False, fill=True)\nplt.tight_layout()\nplt.show()\ndel df_plot","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-04T08:08:37.531242Z","iopub.execute_input":"2026-02-04T08:08:37.531681Z","iopub.status.idle":"2026-02-04T08:09:06.709428Z","shell.execute_reply.started":"2026-02-04T08:08:37.531642Z","shell.execute_reply":"2026-02-04T08:09:06.70825Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# 3. Preprocess data","metadata":{}},{"cell_type":"code","source":"## Helping functions for adding statistical features\n\ndef frq_enc(X, features, freq_encodings=freq_encodings):\n    X_freq = pd.DataFrame()\n    for c in features:\n        X_freq[c] = X[c].map(freq_encodings[c]).astype(float).fillna(0)\n    return X_freq\n\ndef target_stats(X, features, stats, st_type, global_stats=global_stats):\n    X_stat = pd.DataFrame()\n    for c in features:\n        X_stat[c] = X[c].map(stats[c]).fillna(global_stats[st_type])\n    return X_stat","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-04T08:09:06.710927Z","iopub.execute_input":"2026-02-04T08:09:06.711323Z","iopub.status.idle":"2026-02-04T08:09:06.718723Z","shell.execute_reply.started":"2026-02-04T08:09:06.711291Z","shell.execute_reply":"2026-02-04T08:09:06.717323Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"## Define and fit preprocessing pipeline\n\nMAX_FEAT = None # Max number of features after feature selection\n\nrobust_pipeline = Pipeline([('imputer', SimpleImputer(strategy=\"median\")),\n                            ('robust_scaling', RobustScaler())])\npca_pipeline = Pipeline([('imputer', SimpleImputer(strategy=\"median\")),\n                         ('robust_scaling', RobustScaler()), ('pca', PCA(random_state=42))])\nordinal_pipeline = Pipeline([('imputer', SimpleImputer(strategy=\"most_frequent\")),\n                             ('ordinal', OrdinalEncoder(handle_unknown='use_encoded_value', unknown_value=-1, dtype=np.int8))])\nonehot_pipeline = Pipeline([('imputer', SimpleImputer(strategy=\"most_frequent\")), ('onehot', OneHotEncoder())])\nlog_pipeline = Pipeline([('imputer', SimpleImputer(strategy=\"median\")),\n                         ('log_trans', FunctionTransformer(func=lambda x: np.log(x+0.001), feature_names_out='one-to-one')),\n                         ('robust_scaling', RobustScaler())])\nsquare_pipeline = Pipeline([('imputer', SimpleImputer(strategy=\"median\")),\n                            ('square_trans', FunctionTransformer(func=np.square, feature_names_out='one-to-one')),\n                            ('robust_scaling', RobustScaler())])\ncube_pipeline = Pipeline([('imputer', SimpleImputer(strategy=\"median\")),\n                          ('cube_trans', FunctionTransformer(func=lambda x: np.power(x, 3), feature_names_out='one-to-one')),\n                          ('robust_scaling', RobustScaler())])\nsqrt_pipeline = Pipeline([('imputer', SimpleImputer(strategy=\"median\")),\n                          ('sqrt_trans', FunctionTransformer(func=np.sqrt, feature_names_out='one-to-one')),\n                          ('robust_scaling', RobustScaler())])\ncbrt_pipeline = Pipeline([('imputer', SimpleImputer(strategy=\"median\")),\n                          ('cbrt_trans', FunctionTransformer(func=np.cbrt, feature_names_out='one-to-one')),\n                          ('robust_scaling', RobustScaler())])\nkbins_pipeline = Pipeline([('imputer', SimpleImputer(strategy=\"median\")),\n                           ('kbins', KBinsDiscretizer(n_bins=10, strategy='uniform', encode='ordinal', random_state=42)),\n                           ('kbins_cast', FunctionTransformer(lambda X: X.astype(np.uint8), feature_names_out='one-to-one'))])\nfrq_pipeline = Pipeline([('imputer', SimpleImputer(strategy=\"median\")),\n                         ('frq', FunctionTransformer(func=lambda x: frq_enc(x, x.columns), feature_names_out='one-to-one'))])\nmean_pipeline = Pipeline([('imputer', SimpleImputer(strategy=\"median\")),\n                          ('mean', FunctionTransformer(func=lambda x: target_stats(x, x.columns, stats_mean, 'mean'), feature_names_out='one-to-one'))])\ncount_pipeline = Pipeline([('imputer', SimpleImputer(strategy=\"median\")),\n                           ('count', FunctionTransformer(func=lambda x: target_stats(x, x.columns, stats_count, 'count'), feature_names_out='one-to-one'))])\n\n# Preprocessing pipeline\npreprocessing = ColumnTransformer([(\"scaled\", robust_pipeline, num_columns),\n                                   #(\"pca\", pca_pipeline, num_columns),\n                                   (\"cluster\", kbins_pipeline, num_columns),\n                                   (\"frqn\", frq_pipeline, num_columns),\n                                   (\"ordinal\", ordinal_pipeline, cat_columns),\n                                   (\"frqc\", frq_pipeline, cat_columns),\n                                   (\"mean\", mean_pipeline, num_columns+cat_columns),\n                                   (\"count\", count_pipeline, num_columns+cat_columns)]).set_output(transform='pandas')\n\n# Preprocess data\ntrain_prepared = preprocessing.fit_transform(train)\nval_prepared = preprocessing.transform(val)\nprint(f'Number of unfiltered features: {train_prepared.shape[1]}')\n\n# Final feature selection based on XGBoost feature importances\nif MAX_FEAT:\n    xgbr_fs = xgboost.XGBClassifier(device='cpu', random_state=42).fit(train_prepared, train_labels)\n    model_fs = SelectFromModel(xgbr_fs, max_features=MAX_FEAT, threshold=1e-5, prefit=True).fit(train_prepared, train_labels)\n    train_prepared = pd.DataFrame(model_fs.transform(train_prepared), columns=model_fs.get_feature_names_out())\n    val_prepared = pd.DataFrame(model_fs.transform(val_prepared), columns=model_fs.get_feature_names_out())\n    print(f'Number of selected features: {train_prepared.shape[1]}')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-04T08:29:02.148279Z","iopub.execute_input":"2026-02-04T08:29:02.148641Z","iopub.status.idle":"2026-02-04T08:29:07.245644Z","shell.execute_reply.started":"2026-02-04T08:29:02.148612Z","shell.execute_reply":"2026-02-04T08:29:07.244627Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# 4. Define model space for ML methods","metadata":{}},{"cell_type":"code","source":"## Helping function to create parameter grids\n\ndef make_param(param_dict, model='est'):\n    for elem in param_dict.copy():\n        if elem == 'n_components':\n            param_dict['pca'+'__'+elem] = param_dict.pop(elem)\n        else:\n            param_dict[model+'__'+elem] = param_dict.pop(elem)\n    return param_dict","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-03T07:41:11.210308Z","iopub.execute_input":"2026-02-03T07:41:11.210583Z","iopub.status.idle":"2026-02-03T07:41:11.214879Z","shell.execute_reply.started":"2026-02-03T07:41:11.21056Z","shell.execute_reply":"2026-02-03T07:41:11.214083Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"## Machine learning models and their hyperparameter search space\n\nGPU_ACC = True\n\n# Models\nsvc = SVC(kernel='linear', class_weight='balanced')\nrfc = RandomForestClassifier(random_state=42)\nkneigh = KNeighborsClassifier()\ngbc = GradientBoostingClassifier(random_state=42)\nxgb = xgboost.XGBClassifier(objective='binary:logistic', enable_categorical=True, device='cuda' if GPU_ACC else 'cpu',\n                            random_state=42, eval_metric=\"auc\")\nada = AdaBoostClassifier(random_state=42)\nhgbc = HistGradientBoostingClassifier(scoring='roc_auc', class_weight='balanced', random_state=42)\nlgbm = lightgbm.LGBMClassifier(objective='binary', metric='auc', is_unbalance=True, random_state=42,\n                               device ='gpu' if GPU_ACC else 'cpu', verbosity=-1)\ncatc = CatBoostClassifier(eval_metric='AUC', auto_class_weights='Balanced', random_state=123,\n                          task_type='GPU' if GPU_ACC else 'CPU', verbose=False)\nsgdc = SGDClassifier(loss='log_loss', class_weight='balanced', random_state=42)\n\n# Model space\nEstimatorStr = {1: 'svc', 2: 'rfc', 3: 'kneigh', 4: 'gbc', 5: 'xgb', 6: 'ada', 7: 'hgbc', 8: 'lgbm', 9: 'catc', 10: 'sgdc'}\nEstimatorMdl = {1: svc, 2: rfc, 3: kneigh, 4: gbc, 5: xgb, 6: ada, 7: hgbc, 8: lgbm, 9: catc, 10: sgdc}","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-03T07:41:11.21585Z","iopub.execute_input":"2026-02-03T07:41:11.216499Z","iopub.status.idle":"2026-02-03T07:41:11.283497Z","shell.execute_reply.started":"2026-02-03T07:41:11.216476Z","shell.execute_reply":"2026-02-03T07:41:11.282649Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"## Tuned hyperparameter sets\n\n# svc parameter\nparam_single_svc = make_param({}) #\n# rfc parameter\nparam_single_rfc = make_param({}) # \n# kneight parameter\nparam_single_kneigh = make_param({}) # \n# gbc parameter\nparam_single_gbc = make_param({}) # \n# xgb parameter\nparam_single_xgb = make_param({'n_estimators': 8000, 'learning_rate': 0.03, 'early_stopping_rounds': 100,\n                               }) # \n# ada parameter\nparam_single_ada = make_param({}) # \n# hgbc parameter\nparam_single_hgbc = make_param({'max_iter': 8000, 'learning_rate': 0.03, #'max_bins': 160,\n                                }) # \n# lgbm parameter\nparam_single_lgbm = make_param({'n_estimators': 8000, 'learning_rate': 0.03, 'early_stopping_rounds': 100,\n                                }) # \n# catc parameter\nparam_single_catc = make_param({'n_estimators': 8000, 'learning_rate': 0.03,\n                                'early_stopping_rounds': 400, 'max_depth': 8,\n                                }) # \n# sgdc parameter\nparam_single_sgdc = make_param({}) # ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-03T07:41:32.274043Z","iopub.execute_input":"2026-02-03T07:41:32.274557Z","iopub.status.idle":"2026-02-03T07:41:32.279573Z","shell.execute_reply.started":"2026-02-03T07:41:32.274529Z","shell.execute_reply":"2026-02-03T07:41:32.278997Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"## Hyperparameter sets for parameter tuning\n\n# xgb parameter\nparam_grid_xgb = make_param({#'n_estimators': [140, 500, 1000, 1500],\n                             'max_depth': [2, 4, 6, 8],\n                             'learning_rate': [0.1, 0.3, 0.5, 0.7],\n                             'subsample': [0.1, 0.5, 0.9],\n                             'colsample_bytree': [0.1, 0.5, 0.9],\n                             'reg_lambda': [0, 0.1, 1, 10],\n                             'reg_alpha': [0, 0.1, 1, 10],\n                             })\n# hgbc parameter\nparam_grid_hgbc = make_param({#'max_depth': [3, 4, 5, 6, 9, 12],\n                              #'learning_rate': [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8],\n                             'max_bins': [10,20,40,80,160,255]\n                              })\n# lgbm parameter\nparam_grid_lgbm = make_param({'num_leaves': [80,110,120,125,130,135,140,150,180],\n                              'max_depth': [24,29,31,32,33,35,40],\n                              'learning_rate': [0.05,0.1,0.2],\n                              #'n_estimators': [100,200,300],\n                              })\n# catc parameter\nparam_grid_catc = make_param({'n_estimators': [8000], # 4000, 6000, 8000, 10000\n                              'learning_rate': [0.03], # 0.005, 0.01, 0.03, 0.05, 0.1\n                              'max_depth': [4,5,6,7,8], #\n                              'early_stopping_rounds': [100],\n                                #'learning_rate': 0.06, 'max_depth': 2, 'l2_leaf_reg': 0.3\n                                })","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-03T07:41:34.263512Z","iopub.execute_input":"2026-02-03T07:41:34.264127Z","iopub.status.idle":"2026-02-03T07:41:34.269219Z","shell.execute_reply.started":"2026-02-03T07:41:34.2641Z","shell.execute_reply":"2026-02-03T07:41:34.268584Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# 5. Training","metadata":{}},{"cell_type":"code","source":"## Single fitting with tuned parameters or grid search for machine learning methods\n\nTUNING = False # Choose between single fitting or parameter tuning\nFOLDS = 5\nEST_IDS = [9] # Choose model(s) to tune {1: 'svr', 2: 'rfr', 3: 'kneigh', 4: 'gbr', 5: 'xgb', 6: 'ada', 7: 'hgbr', 8: 'lgbm', 9: 'catr', 10: 'sgdr'}\nEST_IDS_W_EARLYSTOPPING = [5,8,9]\nEST_IDS_W_CAT_FEAT = [5,7,8,9]\n\nfor est_id in EST_IDS:\n    start_time = time.time()\n    # Define pipeline\n    pipeline = Pipeline([('est', EstimatorMdl[est_id])])\n\n    # Cross-validation configuration w/wo extended stratification\n    skf = (StratifiedKFold if STRAT else KFold)(n_splits=FOLDS, shuffle=True, random_state=42)\n    cv_gen = skf.split(train_prepared, train['multicat'] if EXTENDED_STRAT else train_labels)\n\n    # Cast categorical features to 'category' for choosen estimators being able to handle it\n    ordinal_columns = make_column_selector(pattern='ordinal|cluster')(train_prepared)\n    train_prepared[ordinal_columns] = train_prepared[ordinal_columns].astype(str).astype('category' if est_id in EST_IDS_W_CAT_FEAT else 'uint8')\n    val_prepared[ordinal_columns] = val_prepared[ordinal_columns].astype(str).astype('category' if est_id in EST_IDS_W_CAT_FEAT else 'uint8')\n    catc.set_params(cat_features=ordinal_columns if est_id in EST_IDS_W_CAT_FEAT else None)\n    \n    # Fitting or tuning on train dataset with k-fold cross-validation\n    param = globals()[f'param_grid_{EstimatorStr[est_id]}' if TUNING else f'param_single_{EstimatorStr[est_id]}']\n    if TUNING:\n        grid = GridSearchCV(pipeline, param, scoring='roc_auc', verbose=1, cv=cv_gen)\n        #grid.fit(cp.array(train_prepared), np.array(train_labels))\n        grid.fit(train_prepared, np.array(train_labels))\n        print(grid.best_params_)\n        print(grid.cv_results_)\n        globals()[f'model1_{EstimatorStr[est_id]}'] = grid.best_estimator_\n    else:\n        for i, (train_index, eval_index) in enumerate(cv_gen):\n            X_train, X_eval = train_prepared.iloc[train_index], train_prepared.iloc[eval_index]\n            y_train, y_eval = train_labels.iloc[train_index], train_labels.iloc[eval_index]\n            pipeline_train = deepcopy(pipeline)\n            pipeline_train.set_params(**param) # , est__random_state=(i+1)*1\n            eval_set = {}\n            if est_id in EST_IDS_W_EARLYSTOPPING:\n                eval_set['est__eval_set'] = [(X_eval, np.array(y_eval))]\n                if est_id==5: eval_set['est__verbose'] = 0\n\n            #pipeline_train.fit(cp.array(X_train), np.array(y_train), **eval_set)\n            pipeline_train.fit(X_train, np.array(y_train), **eval_set)\n            \n            globals()[f'model{i+1}_{EstimatorStr[est_id]}'] = pipeline_train\n            print(f'Estimator: {EstimatorStr[est_id]} of fold {i+1} is fitted')\n            print(f'Elapsed time: {int(time.time() - start_time)} [s]')\n            print('-'*40)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-03T07:41:38.558267Z","iopub.execute_input":"2026-02-03T07:41:38.558827Z","iopub.status.idle":"2026-02-03T07:46:26.719955Z","shell.execute_reply.started":"2026-02-03T07:41:38.558797Z","shell.execute_reply":"2026-02-03T07:46:26.71909Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# 6. Evaluation","metadata":{}},{"cell_type":"code","source":"## Calculate ROC_AUC scores for each estimators and mean ROC_AUC score over all estimators and folds\n\nval_pred = pd.DataFrame()\n\n# ROC_AUC scores for each estimator and fold\nfor est_id in EST_IDS:\n    # Cast categorical features to 'category' for choosen estimators being able to handle it\n    train_prepared[ordinal_columns] = train_prepared[ordinal_columns].astype('category' if est_id in EST_IDS_W_CAT_FEAT else 'int8')\n    val_prepared[ordinal_columns] = val_prepared[ordinal_columns].astype('category' if est_id in EST_IDS_W_CAT_FEAT else 'int8')\n    \n    for i in range(1 if TUNING else FOLDS):\n        # Select proper model\n        pipeline_val = globals()[f'model{i+1}_{EstimatorStr[est_id]}']\n        \n        # Calculate and show scores\n        train_score = roc_auc_score(np.array(train_labels), pipeline_val.predict_proba(train_prepared)[:,1])\n        val_score = roc_auc_score(np.array(val_labels), pipeline_val.predict_proba(val_prepared)[:,1])\n        print(f'Estimator: {EstimatorStr[est_id]} of fold {i+1}')\n        print(f'Train ROC_AUC  score: {train_score}')\n        print(f'Val ROC_AUC  score: {val_score}')\n        print(f'Elapsed time: {int(time.time() - start_time)} [s]')\n        print('-'*40)\n        \n        # Store predictions\n        val_pred[f'pred_{EstimatorStr[est_id]}_{i+1}'] = pipeline_val.predict_proba(val_prepared)[:,1]\n\n# ROC_AUC score on mean value of ensemble predictions for each fold\nval_pred['pred_score'] = val_pred.mean(axis=1)\nval_pred[target] = val_labels\nval_score_avg = roc_auc_score(val_labels, val_pred['pred_score'])\nprint(f'Mean val ROC_AUC score over all estimators and folds: {val_score_avg}')\nprint('-'*40)\nprint('Show predictions and labels of validation dataset: ')\nprint(val_pred)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-03T07:46:26.721272Z","iopub.execute_input":"2026-02-03T07:46:26.721599Z","iopub.status.idle":"2026-02-03T07:46:31.774782Z","shell.execute_reply.started":"2026-02-03T07:46:26.721577Z","shell.execute_reply":"2026-02-03T07:46:31.774104Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"## Show ROC_AUC scores of subcategory subsets\n\nsubcats = []\nsubcat_ROC_AUC_scores  = []\nfor cat in cat_columns:\n    for subcat in np.sort(val[cat].unique()):\n        subcats.append(cat+'_'+str(subcat))\n        val_filtered = val_pred[target][val[cat] == subcat]\n        val_labels_filtered = val_pred['pred_score'][val[cat] == subcat]\n        subcat_ROC_AUC_scores.append(roc_auc_score(val_filtered, val_labels_filtered))\n\n# Create bar chart\nfig1, ax1 = plt.subplots(figsize=(10, 8))\ncmap = plt.get_cmap('viridis')\nrescale = lambda x: (x - np.min(x)) / (np.max(x) - np.min(x))\nnormalized_values = rescale(subcat_ROC_AUC_scores)\nax1.barh(subcats, np.array(subcat_ROC_AUC_scores)-val_score_avg, color=cmap(normalized_values), left=val_score_avg)\nplt.title(f'ROC_AUC scores of subcategory subsets compared to the average ROC_AUC score')\nplt.xlabel('ROC_AUC score')\nplt.ylabel('Subcategories')\nax1.axvline(x=val_score_avg, color='green', linestyle='-.')\nplt.tight_layout()\nax1.tick_params(left=False, bottom=True)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"## Show feature importances\n\nif est_id in [5, 8, 9]:\n    # Sort feature names and their importances\n    categories = (model_fs if MAX_FEAT else preprocessing).get_feature_names_out()\n    values = globals()[f'model1_{EstimatorStr[est_id]}'][-1].feature_importances_+1e-4\n    sorted_values, sorted_categories = zip(*sorted(zip(values,categories), reverse=False))\n    \n    # Plot feature importances\n    fig3, ax3 = plt.subplots(figsize=(10, 8))\n    normalized_values = rescale(np.log(sorted_values))\n    ax3.barh(sorted_categories, sorted_values, color=cmap(normalized_values), log=True)\n    plt.title(f'Feature Importances (Magnitude)')\n    plt.xlabel('Logarithmic importance score')\n    plt.ylabel('Features')\n    plt.tight_layout()\n    ax3.tick_params(left=False, bottom=True)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"## Show worse ROC_AUC scores of multicategory subsets\n\nmulticats = []\nmulticat_ROC_AUC_scores = []\nfor multicat in trainval['multicat_eval'].unique():\n    if len(val_labels[val['multicat_eval'] == multicat].unique()) > 1:\n        multicats.append(multicat)\n        val_filtered = val_pred[target][val['multicat_eval'] == multicat]\n        val_labels_filtered = val_pred['pred_score'][val['multicat_eval'] == multicat]\n        multicat_ROC_AUC_scores.append(roc_auc_score(val_filtered, val_labels_filtered))\nmulticats = strat_encoder_eval.inverse_transform(multicats)\nsorted_multicat_ROC_AUC_scores, sorted_multicats = zip(*sorted(zip(multicat_ROC_AUC_scores,multicats), reverse=True))\n\n# Create bar chart\nfig2, ax2 = plt.subplots(figsize=(10, 8))\nn_top = 10\ncmap = plt.get_cmap('viridis')\nrescale = lambda x: (x - np.min(x)) / (np.max(x) - np.min(x))\nnormalized_values = rescale(sorted_multicat_ROC_AUC_scores[:n_top])\nax2.barh(sorted_multicats[:n_top], np.array(sorted_multicat_ROC_AUC_scores[:n_top])-val_score_avg, color=cmap(normalized_values), left=val_score_avg)\nplt.title(f'Worse {n_top} ROC_AUC scores of multicategory subsets')\nplt.xlabel('ROC_AUC score')\nplt.ylabel('Subcategories')\nplt.tight_layout()\nax2.tick_params(left=False, bottom=True)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# 7. Submission","metadata":{}},{"cell_type":"code","source":"## Test prediction & submission \n\ntest_prepared = preprocessing.transform(test)\nif MAX_FEAT:\n    test_prepared = pd.DataFrame(model_fs.transform(test_prepared), columns=model_fs.get_feature_names_out())\ntest_pred = pd.DataFrame()\nsubmission_df = test[['id']].copy()\n\n# Make predictions for all estimators and folds and take the mean value as prediction\nfor est_id in EST_IDS:\n    # Cast categorical features to 'category' for choosen estimators being able to handle it\n    test_prepared[ordinal_columns] = test_prepared[ordinal_columns].astype(str).astype(\n        'category' if est_id in EST_IDS_W_CAT_FEAT else 'int8')\n    \n    for i in range(1 if TUNING else FOLDS):\n        model_test = globals()[f'model{i+1}_{EstimatorStr[est_id]}']\n        test_pred[f'pred{i+1}_{EstimatorStr[est_id]}'] = model_test.predict_proba(test_prepared)[:,1]\nsubmission_df['exam_score'] = test_pred.mean(axis=1)\n\nsubmission_df.to_csv(\"submission.csv\", index=False)\nprint(\"âœ… submission.csv saved!\")\nsubmission_df","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}