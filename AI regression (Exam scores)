{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.12.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":119082,"databundleVersionId":14993753,"sourceType":"competition"},{"sourceId":13904981,"sourceType":"datasetVersion","datasetId":8762382}],"dockerImageVersionId":31234,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"source":"<a href=\"https://www.kaggle.com/code/kospintr/students-xgb-lightgbm-catb-feature-select-k-fold?scriptVersionId=296043771\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown"},{"cell_type":"markdown","source":"# Overview and main features","metadata":{}},{"cell_type":"markdown","source":"**1. Import packages**\n\n**2. Load and explore data:**\n* Load and split data into train and validation sets in a stratified manner. As reference for the stratification either a calculated multilabel (based on selected features and/or labels) or simply the label can be used (*EXTENDED_STRAT == False*).\n* External dataset can be merged with the train dataset (*ADD_EXTERN_DATA == True*).\n* List of dataset columns including data types and number of non-zero elements.\n* Show number of unique elements of categorical features.\n* Show probability distribution of numerical features.\n* Compare probabilty distribution of numerical features between train and validation sets\n\n**3. Preprocess data:**\n* The dataset includes 2 main types of data: numerical features with unifrom distribution and categorical features.\n* Preprocessing pipeline includes an imputing step (for robustness of pipeline, currently there are no missing values in raw data), a transforming step (for categorical features) and a scaling step (for numerical features).\n* Preprocessed features will be passed into a feature selection model to extract most relevant features. The maximum number of features can be configured (MAX_FEAT).\n* Optionally, one can include different transformations (log/square/cube/sqrt/cbrt) on the numeric features and add them to the pipeline as additional derived features.\n* Optionally, PCA can be added into the numerical pipeline to reduce number of features (*PCA_active == True*).\n\n\n**4. Define model space for ML methods:**\n* Definition of model space with a large number of possible ML methods:\n    * Linear models: SGD\n    * Support vector machine model: SVR\n    * Ensemble models: RandomForest / GradientBoosting / AdaBoost / HistGradientBoosting / XGB / LGBM / CatBoost\n    * Other models: KNeighbors\n\n**5. Training and evaluation:**\n* Definition of model parameter sets and tuner interfaces.\n* Simple fitting or tuning (*TUNING == True*) of choosen models (*EST_IDS*) on train dataset with k-Fold method (*FOLD == k*).\n* Early stopping and the usage of category data type can be configured (*EST_IDS_W_EARLYSTOPPING/EST_IDS_W_CAT_FEAT*).\n\n**6. Evaluation:**\n* Calculate RMSE scores for all estimators on both train and validation dataset.\n* RMSE score on mean value of ensemble predictions for each fold on validation dataset.\n* Show feature importances.\n* Show RMSE scores of subcategory subsets.\n* Show worse RMSE scores of multicategory subsets\n\n**7. Submission:**\n* Predict labels and create submission.csv file.\n* If more than 1 model have been choosen in EST_IDS (Ensemble solution) then the prediction will be an average of the predictions of the single models.","metadata":{}},{"cell_type":"markdown","source":"# 1. Import packages","metadata":{}},{"cell_type":"code","source":"## Import packages\n\n# General purpose modules\nimport time\nfrom copy import deepcopy\nimport warnings\n\n# Data handling and visualization modules\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport cupy as cp\n\n# Skikit-learn preprocessing and evaluation modules\nfrom sklearn.feature_selection import SelectFromModel\nfrom sklearn.model_selection import ShuffleSplit\nfrom sklearn.model_selection import StratifiedShuffleSplit\nfrom sklearn.model_selection import KFold\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.compose import make_column_selector\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.preprocessing import FunctionTransformer\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.preprocessing import OrdinalEncoder\nfrom sklearn.preprocessing import OneHotEncoder\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.preprocessing import KBinsDiscretizer\nfrom sklearn.metrics import mean_squared_error\n\n# Skikit-learn ML modules\nfrom sklearn.decomposition import PCA\nfrom sklearn.svm import SVR\nfrom sklearn.svm import LinearSVR\nfrom sklearn.neighbors import KNeighborsRegressor\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.ensemble import GradientBoostingRegressor\nfrom sklearn.ensemble import HistGradientBoostingRegressor\nfrom sklearn.ensemble import AdaBoostRegressor\nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn.linear_model import SGDRegressor\nfrom sklearn.linear_model import Lasso\nfrom sklearn.linear_model import ElasticNet\nfrom sklearn.linear_model import Ridge\nfrom sklearn.linear_model import LassoLars\n\n# Further ML modules\nimport xgboost as xgboost\nimport lightgbm as lightgbm\nfrom catboost import CatBoostRegressor","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-30T09:04:12.059908Z","iopub.execute_input":"2026-01-30T09:04:12.060711Z","iopub.status.idle":"2026-01-30T09:04:20.470953Z","shell.execute_reply.started":"2026-01-30T09:04:12.060682Z","shell.execute_reply":"2026-01-30T09:04:20.470161Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# 2. Load and explore data","metadata":{}},{"cell_type":"code","source":"## Read csv files\n\nADD_EXTERN_DATA = True\n\ntrainval = pd.read_csv('/kaggle/input/playground-series-s6e1/train.csv')\nextern_data = pd.read_csv('/kaggle/input/exam-score-prediction-dataset/Exam_Score_Prediction.csv')\nif ADD_EXTERN_DATA:\n    trainval = pd.concat([trainval[trainval.columns[1:]], extern_data[trainval.columns[1:]]]\n                         ).reset_index(drop=True).reset_index().rename(columns={'index':'id'})\ntest = pd.read_csv('/kaggle/input/playground-series-s6e1/test.csv')\n\n## Spliting the data into training and validation set\n\nSTRAT = False # Use stratification for data spliting\nEXTENDED_STRAT = False # Stratification is based on multiple features\n\n# Discretization of labels and numeric features\nkbins_label = KBinsDiscretizer(n_bins=20 if EXTENDED_STRAT else 41, encode='ordinal', strategy='quantile', random_state=42)\ntrainval['label_bins'] = kbins_label.fit_transform(trainval[['exam_score']]).astype(np.int16)\nkbins_study_h = KBinsDiscretizer(n_bins=7, encode='ordinal', strategy='quantile', random_state=42)\ntrainval['study_h_bins'] = kbins_study_h.fit_transform(trainval[['study_hours']]).astype(np.int8)\nkbins_class_a = KBinsDiscretizer(n_bins=7, encode='ordinal', strategy='quantile', random_state=42)\ntrainval['class_a_bins'] = kbins_class_a.fit_transform(trainval[['class_attendance']]).astype(np.int8)\nkbins_sleep_h = KBinsDiscretizer(n_bins=7, encode='ordinal', strategy='quantile', random_state=42)\ntrainval['sleep_h_bins'] = (kbins_sleep_h.fit_transform(trainval[['sleep_hours']])).astype(np.int8)\n\n# Determine stratification bins\nstrat_encoder = LabelEncoder()\nstrat_cols = ['sleep_quality', 'study_method', 'facility_rating', 'label_bins']\ntrainval['multicat'] =strat_encoder.fit_transform(trainval[strat_cols].astype(str).agg('_'.join, axis=1))\nsss = StratifiedShuffleSplit(n_splits=1, test_size=0.01, random_state=3) if STRAT else ShuffleSplit(n_splits=1, test_size=0.01, random_state=42)\n\n# Spliting data\ntrain_idx, val_idx = next(sss.split(trainval, trainval['multicat'] if EXTENDED_STRAT else trainval['label_bins']))\ntrain = trainval.iloc[train_idx].reset_index()\nval = trainval.iloc[val_idx].reset_index()\ntrainval_labels = trainval.pop('exam_score')\ntrain_labels = train.pop('exam_score')\nval_labels = val.pop('exam_score')\n\n# Verify sizes\nprint(f\"Total rows:   {len(trainval)}\")\nprint(f\"Dev train:    {len(train)} ({len(train)/len(trainval):.2%})\")\nprint(f\"Dev valid:    {len(val)} ({len(val)/len(trainval):.2%})\")\nprint(f\"Number of unique elements in multicat column: {len(trainval['multicat'].unique())}\")\n\n# Size of stratification bins\ntrainval['multicat'].value_counts()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-30T09:04:20.472331Z","iopub.execute_input":"2026-01-30T09:04:20.472959Z","iopub.status.idle":"2026-01-30T09:04:25.723531Z","shell.execute_reply.started":"2026-01-30T09:04:20.472934Z","shell.execute_reply":"2026-01-30T09:04:25.72295Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"## Explore train dataset\n\nprint('List of dataset columns including data types and number of non-zero elements: ', end='\\n\\n')\ntrain.info()\nprint('-'*80, end='\\n\\n')\n\n# Explore categorical features\ncat_columns = make_column_selector(dtype_include=('object', 'int8'))(train)\nprint('Number of unique elements of categorical features: ', end='\\n\\n')\nfor cat in cat_columns:\n    print(train[cat].value_counts(), end='\\n\\n')\n\n# Explore numerical features\nnum_columns = make_column_selector(dtype_exclude=('object', 'int8'))(train)\nfor col in ['index', 'id', 'label_bins', 'multicat']:\n    num_columns.remove(col)\ntrain[num_columns].hist(bins=100, figsize=(16,10))\nplt.suptitle('Probability distribution of numerical features: ')\nprint('-'*80, end='\\n\\n')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-30T09:04:25.724467Z","iopub.execute_input":"2026-01-30T09:04:25.72473Z","iopub.status.idle":"2026-01-30T09:04:27.366583Z","shell.execute_reply.started":"2026-01-30T09:04:25.724701Z","shell.execute_reply":"2026-01-30T09:04:27.365975Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"## Compare probabilty distribution of numerical features between train and validation sets\n\ndf_plot = pd.concat([train[num_columns].assign(Set='Train'), val[num_columns].assign(Set='Validation')])\ndf_plot = pd.concat([pd.concat([train_labels, val_labels]), df_plot], axis=1)\nwarnings.filterwarnings(\"ignore\", category=FutureWarning, module=\"seaborn\") # Suppress the specific FutureWarning\n\nn_cols = 3\nn_rows = (len(num_columns) + n_cols - 1) // n_cols\nfig, axes = plt.subplots(n_rows, n_cols, figsize=(15, 4 * n_rows))\nfig.suptitle('Feature distributions in train and validation sets')\naxes = axes.flatten()\nfor i in range(len(num_columns), len(axes)):\n    axes[i].axis('off')\nfor i, col in enumerate(df_plot.columns[:-1]):\n    sns.kdeplot(data=df_plot, x=col, ax=axes[i], hue='Set', common_norm=False, fill=True)\nplt.tight_layout()\nplt.show()\ndel df_plot","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-30T09:04:27.368078Z","iopub.execute_input":"2026-01-30T09:04:27.368286Z","iopub.status.idle":"2026-01-30T09:04:42.080486Z","shell.execute_reply.started":"2026-01-30T09:04:27.368266Z","shell.execute_reply":"2026-01-30T09:04:42.079716Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# 3. Preprocess data","metadata":{}},{"cell_type":"code","source":"## Define and fit preprocessing pipeline\n\nPCA_active = False # Activate PCA for numerical features\nMAX_FEAT = 9 # Max number of features after feature selection\n\nminmax_pipeline = Pipeline([('imputer', SimpleImputer(strategy=\"median\")), ('minmax_scaling', MinMaxScaler()), ('pca', PCA(random_state=42))] if PCA_active else\n                           [('imputer', SimpleImputer(strategy=\"median\")), ('minmax_scaling', MinMaxScaler())])\nordinal_pipeline = Pipeline([('imputer', SimpleImputer(strategy=\"most_frequent\")),\n                             ('ordinal', OrdinalEncoder(handle_unknown='use_encoded_value', unknown_value=-1, dtype=np.int8))])\nonehot_pipeline = Pipeline([('imputer', SimpleImputer(strategy=\"most_frequent\")),\n                            ('onehot', OneHotEncoder())])\nlog_pipeline = Pipeline([('imputer', SimpleImputer(strategy=\"median\")),\n                         ('log_trans', FunctionTransformer(func=np.log, feature_names_out='one-to-one')),\n                         ('minmax_scaling', MinMaxScaler())])\nsquare_pipeline = Pipeline([('imputer', SimpleImputer(strategy=\"median\")),\n                            ('square_trans', FunctionTransformer(func=np.square, feature_names_out='one-to-one')),\n                            ('minmax_scaling', MinMaxScaler())])\ncube_pipeline = Pipeline([('imputer', SimpleImputer(strategy=\"median\")),\n                          ('cube_trans', FunctionTransformer(func=lambda x: np.power(x, 3), feature_names_out='one-to-one')),\n                          ('minmax_scaling', MinMaxScaler())])\nsqrt_pipeline = Pipeline([('imputer', SimpleImputer(strategy=\"median\")),\n                          ('sqrt_trans', FunctionTransformer(func=np.sqrt, feature_names_out='one-to-one')),\n                          ('minmax_scaling', MinMaxScaler())])\ncbrt_pipeline = Pipeline([('imputer', SimpleImputer(strategy=\"median\")),\n                          ('cbrt_trans', FunctionTransformer(func=np.cbrt, feature_names_out='one-to-one')),\n                          ('minmax_scaling', MinMaxScaler())])\n\n# Remove not important features\nfor col in []: #'internet_access', 'exam_difficulty', 'gender', 'course', 'age'\n    try:\n        cat_columns.remove(col)\n    except:\n        try:\n            num_columns.remove(col)\n        except:\n            print('All columns have been already removed!')\n            break\n\n# Preprocessing pipeline\npreprocessing = ColumnTransformer([(\"minmax\", minmax_pipeline, num_columns),\n                                   (\"log\", log_pipeline, num_columns),\n                                   (\"square\", square_pipeline, num_columns),\n                                   (\"cube\", cube_pipeline, num_columns),\n                                   (\"sqrt\", sqrt_pipeline, num_columns),\n                                   (\"cbrt\", cbrt_pipeline, num_columns),\n                                   (\"ordinal\", ordinal_pipeline, cat_columns)]).set_output(transform='pandas')\n\n# Preprocess data\ntrain_prepared = preprocessing.fit_transform(train)\nval_prepared = preprocessing.transform(val)\nprint(f'Number of unfiltered features: {train_prepared.shape[1]}')\n\n# Final feature selection based on LightGBM feature importances\nlgbm_fs = lightgbm.LGBMRegressor(device_type ='cpu', random_state=42, verbosity=-1).fit(train_prepared, train_labels)\nxgbr_fs = xgboost.XGBRegressor(device='cpu', random_state=42).fit(train_prepared, train_labels)\nmodel_fs = SelectFromModel(xgbr_fs, max_features=MAX_FEAT, threshold=1e-7, prefit=True).fit(train_prepared, train_labels)\ntrain_prepared = pd.DataFrame(model_fs.transform(train_prepared), columns=model_fs.get_feature_names_out())\nval_prepared = pd.DataFrame(model_fs.transform(val_prepared), columns=model_fs.get_feature_names_out())\nprint(f'Number of selected features: {train_prepared.shape[1]}')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-30T09:04:42.081522Z","iopub.execute_input":"2026-01-30T09:04:42.081886Z","iopub.status.idle":"2026-01-30T09:04:56.669358Z","shell.execute_reply.started":"2026-01-30T09:04:42.081849Z","shell.execute_reply":"2026-01-30T09:04:56.668517Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# 4. Define model space for ML methods","metadata":{}},{"cell_type":"code","source":"## Helping function to create parameter grids\n\ndef make_param(param_dict, model='est'):\n    for elem in param_dict.copy():\n        if elem == 'n_components':\n            param_dict['pca'+'__'+elem] = param_dict.pop(elem)\n        else:\n            param_dict[model+'__'+elem] = param_dict.pop(elem)\n    return param_dict","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-30T09:04:56.670392Z","iopub.execute_input":"2026-01-30T09:04:56.670686Z","iopub.status.idle":"2026-01-30T09:04:56.675188Z","shell.execute_reply.started":"2026-01-30T09:04:56.670654Z","shell.execute_reply":"2026-01-30T09:04:56.674359Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"## Machine learning models and their hyperparameter search space\n\nGPU_ACC = True\n\n# Models\nsvr = SVR(kernel='linear')\nrfr = RandomForestRegressor(random_state=42)\nkneigh = KNeighborsRegressor()\ngbr = GradientBoostingRegressor(random_state=42)\nxgb = xgboost.XGBRegressor(enable_categorical=True, device='cuda' if GPU_ACC else 'cpu', eval_metric=\"rmse\", random_state=42)\nada = AdaBoostRegressor(random_state=42)\nhgbr = HistGradientBoostingRegressor(scoring='neg_root_mean_squared_error', random_state=42)\nlgbm = lightgbm.LGBMRegressor(metric='rmse', random_state=42, device ='gpu' if GPU_ACC else 'cpu', verbosity=-1)\ncatr = CatBoostRegressor(eval_metric='RMSE', random_state=42, task_type='GPU' if GPU_ACC else 'CPU', verbose=False)\nsgdr = SGDRegressor(random_state=42)\n\n# Model space\nEstimatorStr = {1: 'svr', 2: 'rfr', 3: 'kneigh', 4: 'gbr', 5: 'xgb', 6: 'ada', 7: 'hgbr', 8: 'lgbm', 9: 'catr', 10: 'sgdr'}\nEstimatorMdl = {1: svr, 2: rfr, 3: kneigh, 4: gbr, 5: xgb, 6: ada, 7: hgbr, 8: lgbm, 9: catr, 10: sgdr}","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-30T09:04:56.676329Z","iopub.execute_input":"2026-01-30T09:04:56.676675Z","iopub.status.idle":"2026-01-30T09:04:56.703953Z","shell.execute_reply.started":"2026-01-30T09:04:56.676621Z","shell.execute_reply":"2026-01-30T09:04:56.703177Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"## Training parameters\n\n# Tuned hyperparameter sets\n# svr parameter\nparam_single_svr = make_param({#'C': 107, 'gamma': 0.00082, 'kernel': 'rbf', #'n_components': 300\n                               }) #\n# rfr parameter\nparam_single_rfr = make_param({#'n_estimators': 400, 'max_depth': 12, 'max_leaf_nodes': 100, 'min_samples_split': 3,\n                               }) #\n# kneight parameter\nparam_single_kneigh = make_param({#'n_estimators': 400, 'max_depth': 12, 'max_leaf_nodes': 100, 'min_samples_split': 3,\n                                  }) #\n# gbcr parameter\nparam_single_gbr = make_param({#'n_estimators': 300, 'max_depth': 4, 'learning_rate': 0.05, 'loss': 'huber', 'min_samples_split': 3,\n                               }) #\n# xgb parameter\nparam_single_xgb = make_param({'n_estimators': 10000, 'learning_rate': 0.003,\n                               'max_depth': 18, 'min_child_weight': 1,\n                               'subsample': 1.0, 'colsample_bytree': 0.4, 'colsample_bynode': 1.0,\n                               'reg_lambda': 7, 'reg_alpha': 250, 'min_split_loss': 1.5, 'tree_method': 'hist', \n                               #'early_stopping_rounds': 100,                               \n                               }) #\n# ada parameter\nparam_single_ada = make_param({#'estimator': DecisionTreeRegressor(max_depth=180, criterion='squared_error', max_leaf_nodes=150, min_samples_split=4),\n                               #'n_estimators': 300, 'learning_rate': 0.75, 'loss': 'square',\n                               }) #\n# hgbr parameter\nparam_single_hgbr = make_param({#'max_iter': 1000, 'n_iter_no_change': 100,\n                                }) #\n# lgbm parameter\nparam_single_lgbm = make_param({#'num_iterations': 600, 'learning_rate': 0.025, 'max_depth': 11, 'num_leaves': 350, 'min_data_in_leaf': 75, \n                                }) #\n# catr parameter\nparam_single_catr = make_param({#'iterations': 20000, 'learning_rate': 0.01, 'depth': 4, 'subsample': 0.8,\n                                #'early_stopping_rounds': 200,\n                                }) #\n# sgdr parameter\nparam_single_sgdr = make_param({#'learning_rate': 0.06, 'depth': 2, 'l2_leaf_reg': 0.3\n                                }) #","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-30T09:04:56.704778Z","iopub.execute_input":"2026-01-30T09:04:56.705033Z","iopub.status.idle":"2026-01-30T09:04:56.718195Z","shell.execute_reply.started":"2026-01-30T09:04:56.705011Z","shell.execute_reply":"2026-01-30T09:04:56.717547Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"## Hyperparameter sets for parameter tuning\n\n# xgb parameter\nparam_grid_xgb = make_param({#'n_estimators': [8000, 10000, 12000],\n                             #'learning_rate': [0.001, 0.003, 0.005],\n                             #'max_depth': [11], 'min_child_weight': [400], 'subsample': [1.0], 'colsample_bytree': [0.4], 'colsample_bynode': [1.0],\n                             'max_depth': [16,17,18], #7,9,11,12,13,14,15,17,19\n                             #'min_child_weight': [350, 400, 450], #350, 400, 450\n                             'subsample': [1.0], #0.8, 1.0\n                             'colsample_bytree': [0.4], #0.3, 0.4, 0.5\n                             'colsample_bynode': [1.0], #0.8, 1.0\n                             'reg_lambda': [7], #6, 7, 8\n                             'reg_alpha': [200, 250, 300], #150, 200, 250\n                             'min_split_loss': [1, 1.5, 2], #1, 1.5, 2\n                             'tree_method': ['hist'],                             \n                               }) #\n# lgbm parameter\nparam_grid_lgbm = make_param({'num_iterations': [600],\n                              'learning_rate': [0.025],\n                              'max_depth': [11],\n                              'num_leaves': [350],\n                              'min_data_in_leaf': [75],\n                              })\n# catr parameter\nparam_grid_catr = make_param({'n_estimators': [20000],\n                             'learning_rate': [0.005, 0.01, 0.02],\n                             'max_depth': [4, 7],\n                             'subsample': [0.4, 0.6, 0.8],\n                             'bootstrap_type': ['Bernoulli', 'MVS', 'Poisson']\n                               }) #","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-30T10:19:48.112773Z","iopub.execute_input":"2026-01-30T10:19:48.113358Z","iopub.status.idle":"2026-01-30T10:19:48.118854Z","shell.execute_reply.started":"2026-01-30T10:19:48.113331Z","shell.execute_reply":"2026-01-30T10:19:48.118072Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# 5. Training","metadata":{}},{"cell_type":"code","source":"## Single fitting with tuned parameters or grid search for machine learning methods\n\nTUNING = False # Choose between single fitting or parameter tuning\nFOLDS = 8\nEST_IDS = [5] # Choose model(s) to tune {1: 'svr', 2: 'rfr', 3: 'kneigh', 4: 'gbr', 5: 'xgb', 6: 'ada', 7: 'hgbr', 8: 'lgbm', 9: 'catr', 10: 'sgdr'}\nEST_IDS_W_EARLYSTOPPING = []\nEST_IDS_W_CAT_FEAT = [5,9]\n\nfor est_id in EST_IDS:\n    start_time = time.time()\n    # Define pipeline w/wo PCA\n    pipeline = Pipeline([('est', EstimatorMdl[est_id])])\n\n    # Cross-validation configuration w/wo extended stratification\n    skf = StratifiedKFold(n_splits=FOLDS, shuffle=True, random_state=42) if STRAT else KFold(n_splits=FOLDS, shuffle=True, random_state=42)\n    cv_gen = skf.split(train_prepared, train['multicat'] if EXTENDED_STRAT else train['label_bins'])\n\n    # Cast categorical features to 'category' for choosen estimators being able to handle it\n    ordinal_columns = make_column_selector(pattern='ordinal')(train_prepared)\n    train_prepared[ordinal_columns] = train_prepared[ordinal_columns].astype(str).astype('category' if est_id in EST_IDS_W_CAT_FEAT else 'int8')\n    val_prepared[ordinal_columns] = val_prepared[ordinal_columns].astype(str).astype('category' if est_id in EST_IDS_W_CAT_FEAT else 'int8')\n    catr.set_params(cat_features=ordinal_columns if est_id in EST_IDS_W_CAT_FEAT else None)\n    \n    # Fitting or tuning on train dataset with k-fold cross-validation\n    param = globals()[f'param_grid_{EstimatorStr[est_id]}' if TUNING else f'param_single_{EstimatorStr[est_id]}']\n    if TUNING:\n        grid = GridSearchCV(pipeline, param, scoring='neg_root_mean_squared_error', verbose=3, cv=cv_gen)\n        #grid.fit(cp.array(train_prepared), np.array(train_labels))\n        grid.fit(train_prepared, np.array(train_labels))\n        print(grid.best_params_)\n        print(grid.cv_results_)\n        globals()[f'model1_{EstimatorStr[est_id]}'] = grid.best_estimator_\n    else:\n        for i, (train_index, eval_index) in enumerate(cv_gen):\n            X_train, X_eval = train_prepared.iloc[train_index], train_prepared.iloc[eval_index]\n            y_train, y_eval = train_labels.iloc[train_index], train_labels.iloc[eval_index]\n            pipeline_train = deepcopy(pipeline)\n            pipeline_train.set_params(**param) # , est__random_state=(i+1)*1\n            eval_set = {}\n            if est_id in EST_IDS_W_EARLYSTOPPING:\n                eval_set['est__eval_set'] = [(X_eval, np.array(y_eval))]\n                if est_id==5: eval_set['est__verbose'] = 0\n\n            #pipeline_train.fit(cp.array(X_train), np.array(y_train), **eval_set)\n            pipeline_train.fit(X_train, np.array(y_train), **eval_set)\n            \n            globals()[f'model{i+1}_{EstimatorStr[est_id]}'] = pipeline_train\n            print(f'Estimator: {EstimatorStr[est_id]} of fold {i+1} is fitted')\n            print(f'Elapsed time: {int(time.time() - start_time)} [s]')\n            print('-'*40)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-30T10:19:52.609746Z","iopub.execute_input":"2026-01-30T10:19:52.610318Z","iopub.status.idle":"2026-01-30T10:29:22.37722Z","shell.execute_reply.started":"2026-01-30T10:19:52.610292Z","shell.execute_reply":"2026-01-30T10:29:22.376675Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# 6. Evaluation","metadata":{}},{"cell_type":"code","source":"## Calculate RMSE scores for each estimators and mean RMSE score over all estimators and folds\n\nval_pred = pd.DataFrame()\n\n# RMSE scores for each estimator and fold\nfor est_id in EST_IDS:\n    # Cast categorical features to 'category' for choosen estimators being able to handle it\n    train_prepared[ordinal_columns] = train_prepared[ordinal_columns].astype('category' if est_id in EST_IDS_W_CAT_FEAT else 'int8')\n    val_prepared[ordinal_columns] = val_prepared[ordinal_columns].astype('category' if est_id in EST_IDS_W_CAT_FEAT else 'int8')\n    \n    for i in range(1 if TUNING else FOLDS):\n        # Select proper model\n        pipeline_val = globals()[f'model{i+1}_{EstimatorStr[est_id]}']\n        \n        # Calculate and show scores\n        train_score = mean_squared_error(train_labels, pipeline_val.predict(train_prepared))**(1/2)\n        val_score = mean_squared_error(val_labels, pipeline_val.predict(val_prepared))**(1/2)\n        print(f'Estimator: {EstimatorStr[est_id]} of fold {i+1}')\n        print(f'Train RMSE score: {train_score}')\n        print(f'Val RMSE score: {val_score}')\n        print(f'Elapsed time: {int(time.time() - start_time)} [s]')\n        print('-'*40)\n        \n        # Store predictions\n        val_pred[f'pred_{EstimatorStr[est_id]}_{i+1}'] = pipeline_val.predict(val_prepared)\n\n# RMSE score on mean value of ensemble predictions for each fold\nval_pred['pred_score'] = val_pred.mean(axis=1).round(1)\nval_pred['exam_score'] = val_labels\nval_score_avg = mean_squared_error(val_labels, val_pred['pred_score'])**(1/2)\nprint(f'Mean val RMSE score over all estimators and folds: {val_score_avg}')\nprint('-'*40)\nprint('Show predictions and labels of validation dataset: ')\nprint(val_pred)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-30T10:29:48.823914Z","iopub.execute_input":"2026-01-30T10:29:48.824192Z","iopub.status.idle":"2026-01-30T10:29:49.011492Z","shell.execute_reply.started":"2026-01-30T10:29:48.824169Z","shell.execute_reply":"2026-01-30T10:29:49.010855Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"## Show RMSE scores of subcategory subsets\n\nsubcats = []\nsubcat_RMSE_scores = []\nfor cat in cat_columns:\n    for subcat in np.sort(val[cat].unique()):\n        subcats.append(cat+'_'+str(subcat))\n        val_filtered = val_pred['exam_score'][val[cat] == subcat]\n        val_labels_filtered = val_pred['pred_score'][val[cat] == subcat]\n        subcat_RMSE_scores.append(mean_squared_error(val_labels_filtered, val_filtered)**(1/2))\n\n# Create bar chart\nfig1, ax1 = plt.subplots(figsize=(10, 8))\ncmap = plt.get_cmap('viridis')\nrescale = lambda x: (x - np.min(x)) / (np.max(x) - np.min(x))\nnormalized_values = rescale(subcat_RMSE_scores)\nax1.barh(subcats, np.array(subcat_RMSE_scores)-val_score_avg, color=cmap(normalized_values), left=val_score_avg)\nplt.title(f'RMSE scores of subcategory subsets compared to the average RMSE score')\nplt.xlabel('RMSE score')\nplt.ylabel('Subcategories')\nax1.axvline(x=val_score_avg, color='green', linestyle='-.')\nplt.tight_layout()\nax1.tick_params(left=False, bottom=True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-25T15:39:02.817161Z","iopub.execute_input":"2026-01-25T15:39:02.817373Z","iopub.status.idle":"2026-01-25T15:39:03.353173Z","shell.execute_reply.started":"2026-01-25T15:39:02.817353Z","shell.execute_reply":"2026-01-25T15:39:03.352376Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"## Show worse RMSE scores of multicategory subsets\n\nmulticats = []\nmulticat_RMSE_scores = []\nfor multicat in trainval['multicat'].unique():\n    if (val['multicat'] == multicat).sum() > 0:\n        multicats.append(multicat)\n        val_filtered = val_pred['exam_score'][val['multicat'] == multicat]\n        val_labels_filtered = val_pred['pred_score'][val['multicat'] == multicat]\n        multicat_RMSE_scores.append(mean_squared_error(val_labels_filtered, val_filtered)**(1/2))\nmulticats = strat_encoder.inverse_transform(multicats)\nsorted_multicat_RMSE_scores, sorted_multicats = zip(*sorted(zip(multicat_RMSE_scores,multicats), reverse=True))\n\n# Create bar chart\nfig2, ax2 = plt.subplots(figsize=(10, 8))\nn_top = 33\ncmap = plt.get_cmap('viridis')\nrescale = lambda x: (x - np.min(x)) / (np.max(x) - np.min(x))\nnormalized_values = rescale(sorted_multicat_RMSE_scores[:n_top])\nax2.barh(sorted_multicats[:n_top], np.array(sorted_multicat_RMSE_scores[:n_top])-val_score_avg, color=cmap(normalized_values), left=val_score_avg)\nplt.title(f'Worse {n_top} RMSE scores of multicategory subsets')\nplt.xlabel('RMSE score')\nplt.ylabel('Subcategories')\nplt.tight_layout()\nax2.tick_params(left=False, bottom=True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-25T15:39:03.354809Z","iopub.execute_input":"2026-01-25T15:39:03.355112Z","iopub.status.idle":"2026-01-25T15:39:04.520841Z","shell.execute_reply.started":"2026-01-25T15:39:03.355089Z","shell.execute_reply":"2026-01-25T15:39:04.520175Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"## Predictions and labels of multicategory subset with worse RMSE score\n\nworse_sleep_quality, worse_study_method, worse_facility_rating, worse_label_bins = sorted_multicats[0].split('_')\ntrain_labels[(train['sleep_quality'] == worse_sleep_quality) &\n             (train['study_method'] == worse_study_method) &\n             (train['facility_rating'] == worse_facility_rating)].hist(bins=7)\nplt.title('Label distribution of multicategory subset with worse RMSE score ')\nprint(val_pred[['exam_score', 'pred_score']][(val['sleep_quality'] == worse_sleep_quality) &\n                                             (val['study_method'] == worse_study_method) &\n                                             (val['facility_rating'] == worse_facility_rating) &\n                                             (val['label_bins'] == int(worse_label_bins))])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-25T15:39:04.521807Z","iopub.execute_input":"2026-01-25T15:39:04.522086Z","iopub.status.idle":"2026-01-25T15:39:04.819181Z","shell.execute_reply.started":"2026-01-25T15:39:04.522061Z","shell.execute_reply":"2026-01-25T15:39:04.818597Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"## Show feature importances\n\nif est_id in [5, 8]:\n    # Sort feature names and their importances\n    categories = globals()[f'model1_{EstimatorStr[est_id]}'][0].get_feature_names_out() if PCA_active else model_fs.get_feature_names_out()\n    values = globals()[f'model1_{EstimatorStr[est_id]}'][-1].feature_importances_+1e-4\n    sorted_values, sorted_categories = zip(*sorted(zip(values,categories), reverse=False))\n    \n    # Plot feature importances\n    fig3, ax3 = plt.subplots(figsize=(10, 8))\n    normalized_values = rescale(np.log(sorted_values))\n    ax3.barh(sorted_categories, sorted_values, color=cmap(normalized_values), log=True)\n    plt.title(f'Feature Importances (Magnitude)')\n    plt.xlabel('Logarithmic importance score')\n    plt.ylabel('Features')\n    plt.tight_layout()\n    ax3.tick_params(left=False, bottom=True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-25T15:39:52.784765Z","iopub.execute_input":"2026-01-25T15:39:52.785369Z","iopub.status.idle":"2026-01-25T15:39:53.183803Z","shell.execute_reply.started":"2026-01-25T15:39:52.785341Z","shell.execute_reply":"2026-01-25T15:39:53.183137Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# 7. Submission","metadata":{}},{"cell_type":"code","source":"## Test prediction & submission \n\ntest['study_h_bins'] = kbins_study_h.transform(test[['study_hours']]).astype(np.int8)\ntest['class_a_bins'] = kbins_class_a.transform(test[['class_attendance']]).astype(np.int8)\ntest['sleep_h_bins'] = kbins_sleep_h.transform(test[['sleep_hours']]).astype(np.int8)\ntest_prepared = preprocessing.transform(test)\ntest_prepared = pd.DataFrame(model_fs.transform(test_prepared), columns=model_fs.get_feature_names_out())\ntest_pred = pd.DataFrame()\nsubmission_df = test[['id']].copy()\n\n# Make predictions for all estimators and folds and take the mean value as prediction\nfor est_id in EST_IDS:\n    # Cast categorical features to 'category' for choosen estimators being able to handle it\n    test_prepared[ordinal_columns] = test_prepared[ordinal_columns].astype(str).astype(\n        'category' if est_id in EST_IDS_W_CAT_FEAT else 'int8')\n    \n    for i in range(1 if TUNING else FOLDS):\n        model_test = globals()[f'model{i+1}_{EstimatorStr[est_id]}']\n        test_pred[f'pred{i+1}_{EstimatorStr[est_id]}'] = model_test.predict(test_prepared)\nsubmission_df['exam_score'] = test_pred.mean(axis=1).clip(lower=19.599, upper=100)\n\nsubmission_df.to_csv(\"submission.csv\", index=False)\nprint(\"âœ… submission.csv saved!\")\nsubmission_df","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-24T13:21:09.233863Z","iopub.execute_input":"2026-01-24T13:21:09.234408Z","iopub.status.idle":"2026-01-24T13:21:14.864271Z","shell.execute_reply.started":"2026-01-24T13:21:09.234381Z","shell.execute_reply":"2026-01-24T13:21:14.863664Z"}},"outputs":[],"execution_count":null}]}